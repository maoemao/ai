# #LangChainæ•´åˆç»´åŸºç™¾ç§‘å®ç°ç½‘é¡µé—®ç­”   
ä¸‹é¢ç»™ä½ ä¸€å¥— **LangChain æ•´åˆç»´åŸºç™¾ç§‘ï¼ˆWikipediaï¼‰å®ç°ç½‘é¡µé—®ç­”çš„å®Œæ•´æ–¹æ¡ˆ**ï¼Œä»ä¾èµ–ã€æ£€ç´¢å™¨ã€RAG æµæ°´çº¿ã€åˆ°å¢å¼ºæ£€ç´¢ã€æœ€ç»ˆé—®ç­”æ¥å£ï¼Œå…¨æµç¨‹éƒ½è®²æ¸…æ¥šï¼Œè®©ä½ ç›´æ¥èƒ½åœ¨é¡¹ç›®é‡Œç”¨ã€‚  
å†…å®¹æ˜¯å®æ‰“å®çš„å·¥ç¨‹çº§å®ç°ï¼Œè€Œä¸æ˜¯ä¸€å †æ¦‚å¿µã€‚  
  
## ğŸš€** ä¸€ã€æ•´ä½“èƒ½åŠ›ï¼šä½ èƒ½å®ç°ä»€ä¹ˆï¼Ÿ**  
**LangChain + Wikipedia æ•°æ®æº + LLM = ä»»æ„ç»´åŸºé¡µé¢é—®ç­”ç³»ç»Ÿ**  
åŒ…æ‹¬ï¼š  
* æŒ‡å®šæ¡ç›®é—®ç­”  
* è‡ªåŠ¨æœç´¢ç»´åŸºè¯æ¡  
* å¤šè½®å¯¹è¯  
* RAG æ£€ç´¢å¢å¼º  
* ç½‘é¡µé—®ç­”ï¼ˆæå–æ­£æ–‡ï¼‰  
* è‡ªå®šä¹‰åˆ†ç‰‡ã€rerankã€ä¸Šä¸‹æ–‡å‹ç¼©  
  
## ğŸ§±** äºŒã€å®‰è£…ä¾èµ–**  
```
pip install langchain wikipedia
pip install langchain-openai   # å¦‚æœä½ ç”¨ DeepSeek / OpenAI

```
  
## ğŸ§²** ä¸‰ã€ç»´åŸºç™¾ç§‘åŠ è½½å™¨ï¼ˆæœ€åŸºç¡€ï¼‰**  
LangChain å·²å†…ç½® WikipediaLoaderã€‚  
```
from langchain.document_loaders import WikipediaLoader

docs = WikipediaLoader(query="å¤§è¯­è¨€æ¨¡å‹", load_max_docs=3).load()
for d in docs:
    print(d.metadata)

```
ä½ ä¼šå¾—åˆ°ï¼š  
```
{
  "title": "å¤§è¯­è¨€æ¨¡å‹",
  "source": "wikipedia",
  "url": "https://zh.wikipedia.org/wiki/..."
}

```
å¹¶ä¸” page_content å·²è‡ªåŠ¨æå–å¥½äº†æ­£æ–‡ã€‚  
  
## âœ‚ï¸** å››ã€åˆ†ç‰‡ï¼ˆRAG çš„å…³é”®æ­¥éª¤ï¼‰**  
ç»´åŸºç™¾ç§‘é¡µéƒ½å¾ˆé•¿ï¼Œå¿…é¡»åˆ‡ç‰‡ã€‚  
```
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = splitter.split_documents(docs)

```
  
## ğŸ”®** äº”ã€æ„å»ºå‘é‡åº“ï¼ˆChroma æœ€é€‚åˆå¼€å‘ï¼‰**  
```
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

embed = OpenAIEmbeddings(model="text-embedding-3-large")
vectorstore = Chroma.from_documents(
    chunks, embed, persist_directory="./wiki_db"
)

```
è‹¥ç”¨ DeepSeek Embeddingï¼Œåªè¦æ¢æ¨¡å‹åå³å¯ã€‚  
  
## ğŸ”** å…­ã€æ„å»º Retrieverï¼ˆç½‘é¡µé—®ç­”æ ¸å¿ƒï¼‰**  
```
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 5}
)

```
è¿™æ˜¯æœ€åŸºç¡€ç‰ˆæœ¬ã€‚  
  
## ğŸ¤–** ä¸ƒã€æ„å»º LLMï¼ˆDeepSeek / OpenAIï¼‰**  
ï¼ˆDeepSeek ç¤ºä¾‹ï¼‰  
```
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="deepseek-chat",
    api_key="ä½ çš„key",
    temperature=0.2
)

```
  
## ğŸ”—** å…«ã€æ„å»º RAGï¼ˆä¸»é“¾è·¯ï¼‰**  
```
from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)

```
ä½¿ç”¨ï¼š  
```
result = qa("ä»‹ç»ä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„ä¸»è¦ç‰¹ç‚¹")
print(result)

```
  
## ğŸš€** ä¹ã€é«˜é˜¶ï¼šè‡ªåŠ¨æœç´¢ç»´åŸºç™¾ç§‘ï¼ˆæœç´¢ + åŠ è½½ + RAG å…¨è‡ªåŠ¨ï¼‰**  
å¦‚æœç”¨æˆ·ä¸çŸ¥é“å…·ä½“è¯æ¡ï¼Œä½ å¯ä»¥ç”¨ç»´åŸºæŸ¥è¯¢ API è‡ªåŠ¨æœç´¢ã€‚  
```
def wiki_query(question):
    from langchain.document_loaders import WikipediaLoader

    # è‡ªåŠ¨æœç´¢
    docs = WikipediaLoader(
        query=question, 
        load_max_docs=3, 
        lang="zh"
    ).load()

    chunks = splitter.split_documents(docs)

    # æ„å»ºä¸´æ—¶å‘é‡åº“
    vectorstore = Chroma.from_documents(chunks, embed)

    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff"
    )
    return qa(question)

```
è°ƒç”¨ï¼š  
```
print(wiki_query("ä»€ä¹ˆæ˜¯è¶…å¯¼ä½“ï¼Ÿ"))

```
ä½ å·²ç»æ‹¥æœ‰ä¸€ä¸ª****å…¨è‡ªåŠ¨ç»´åŸºç™¾ç§‘é—®ç­”ç³»ç»Ÿ****ã€‚  
  
## ğŸ§ ** åã€å†å‡çº§ï¼ˆå¼ºçƒˆæ¨èï¼‰ï¼šçŸ¥è¯†å¢å¼ºæ£€ç´¢å™¨**  
## **1ï¼‰MultiQueryRetrieverï¼ˆå¤šæŸ¥è¯¢å¢å¼ºï¼‰**  
```
from langchain.retrievers.multi_query import MultiQueryRetriever

retriever = MultiQueryRetriever.from_llm(
    retriever=retriever, llm=llm
)

```
é€‚ç”¨äºç²¾åº¦ä¸è¶³æ—¶ã€‚  
  
## **2ï¼‰Reranker ç²¾æ’ï¼ˆBGE-Rerankerï¼‰**  
```
from langchain.retrievers.context_compressor import ContextualCompressionRetriever
from langchain_community.document_transformers import BGERerank

reranker = BGERerank(top_n=3)
compression = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=retriever
)

```
LLM çœ‹åˆ°çš„ context æ›´ç²¾å‡†ã€çŸ­ã€è´¨é‡é«˜ã€‚  
  
## **3ï¼‰LLM-based å‹ç¼©ï¼ˆæ·±åº¦æ‘˜è¦ï¼‰**  
```
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)
compression = ContextualCompressionRetriever(
    base_retriever=retriever,
    base_compressor=compressor
)

```
è¶…é•¿æ–‡æ¡£é—®ç­”ç‰¹åˆ«æœ‰æ•ˆã€‚  
  
## ğŸ·ï¸** åä¸€ã€æ„å»ºå®Œæ•´ Web æœåŠ¡ï¼ˆFastAPI ç¤ºä¾‹ï¼‰**  
```
from fastapi import FastAPI
app = FastAPI()

@app.post("/ask")
def ask(question: str):
    return {"answer": wiki_query(question)}

```
ç›´æ¥å¯éƒ¨ç½²æˆç½‘é¡µé—®ç­” APIã€‚  
  
## ğŸ”¥** åäºŒã€å®æˆ˜å»ºè®®ï¼ˆå…³é”®ï¼‰**  
1. **ç»´åŸºç™¾ç§‘å†…å®¹éå¸¸é•¿** â†’ å¿…é¡»åˆ‡ç‰‡ + å‹ç¼©  
2. **è‡ªåŠ¨æœç´¢å¾ˆé‡è¦** â†’ ç”¨æˆ·æé—®é€šå¸¸ä¸æ˜¯è¯æ¡å  
3. **Hybrid æ£€ç´¢èƒœç‡æ›´é«˜** â†’ BM25 + å‘é‡  
4. **Rerank æå‡æœ€æ˜æ˜¾**  
5. **ä¸Šä¸‹æ–‡å‹ç¼©èƒ½æ˜¾è‘—é™ä½ token ç”¨é‡**  
  
## ğŸ“Œ** æ€»ç»“ï¼ˆç»™ä½ ä¸€å¥æœ€å®ç”¨çš„ï¼‰**  
****æ•´åˆç»´åŸºç™¾ç§‘ = WikipediaLoader + åˆ†ç‰‡ + å‘é‡åº“ + RAGã€‚â€¨çœŸæ­£æ•ˆæœå–å†³äºï¼šå¤šæŸ¥è¯¢æ£€ç´¢ + rerank + å‹ç¼© + prompt è®¾è®¡ã€‚****  
  
å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥ç»™ä½ ï¼š  
## âœ”** ã€Šç»´åŸºç™¾ç§‘é—®ç­”ç³»ç»Ÿå®Œæ•´é¡¹ç›®æ¨¡æ¿ï¼ˆç›®å½• + ä»£ç ï¼‰ã€‹**  
## âœ”** ã€Šç»´åŸºæ£€ç´¢ + DeepSeek + Rerank çš„æœ€ä¼˜æ¶æ„å›¾ã€‹**  
## âœ”** ã€Šå¯ä»¥ç›´æ¥éƒ¨ç½²çš„ Web / Streamlit ç½‘é¡µç‰ˆç»´åŸº RAGã€‹**  
ä½ è¦å“ªä¸ªï¼Ÿ  

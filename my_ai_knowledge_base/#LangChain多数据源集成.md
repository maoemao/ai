# #LangChainå¤šæ•°æ®æºé›†æˆ   
ä¸‹é¢æˆ‘ç»™ä½ ä¸€ä»½ **LangChain å¤šæ•°æ®æºé›†æˆï¼ˆMulti-Source RAG / Agentï¼‰å®Œæ•´å·¥ç¨‹æ–¹æ¡ˆ**ï¼Œä»æ¶æ„ â†’ æ¨¡å—æ‹†è§£ â†’ å¤šæºæ£€ç´¢ â†’ å¤šæºåˆå¹¶ â†’ æœ€ä½³å®è·µï¼Œä¸€å¥—è®²é€ï¼Œè®©ä½ èƒ½åœ¨ä¼ä¸šçº§é¡¹ç›®é‡ŒçœŸæ­£è½åœ°ã€‚  
å†…å®¹ç›´è¯´é‡ç‚¹ï¼Œä¸ç©è™šçš„ã€‚  
  
## ğŸš€** ä¸€ã€ä½ è¦å®ç°çš„èƒ½åŠ›æ˜¯ä»€ä¹ˆï¼Ÿ**  
ç”¨æˆ·é—®ä¸€å¥ï¼š  
â€œç»™æˆ‘æ€»ç»“ä¸‹æˆ‘ä»¬ 2024 é”€å”®æŠ¥å‘Šçš„å…³é”®è¶‹åŠ¿ï¼Œå¹¶ç»“åˆè¡Œä¸šå…¬å¼€æ•°æ®åˆ†æåŸå› ã€‚â€  
ç³»ç»Ÿè¦èƒ½è‡ªåŠ¨ä»å¤šä¸ªæ•°æ®æºå–èµ„æ–™ï¼š  
* ğŸ“„ ä¼ä¸šæ–‡æ¡£ï¼ˆPDF/Word/Markdownï¼‰  
* ğŸ“Š æ•°æ®åº“ï¼ˆSQLï¼‰  
* ğŸŒ ç½‘é¡µï¼ˆAPI/çˆ¬è™«ï¼‰  
* ğŸ§  å†…ç½®çŸ¥è¯†åº“ï¼ˆçŸ¥è¯†ç‚¹/è§„åˆ™ï¼‰  
* ğŸ” ä¼ ç»Ÿæœç´¢ï¼ˆBing/Googleï¼‰  
* ğŸ›ï¸ ç¬¬ä¸‰æ–¹ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚ Wikipedia / Finance APIï¼‰  
å¹¶è‡ªåŠ¨ï¼š  
1. æ£€ç´¢ â†’ è¿‡æ»¤  
2. æ’åº â†’ rerank  
3. å‹ç¼© â†’ æ‹¼æ¥ä¸Šä¸‹æ–‡  
4. ç”Ÿæˆåˆ†æ  
è¿™å°±æ˜¯ LangChain çš„ ****å¤šæ•°æ®æºæ•´åˆèƒ½åŠ›ï¼ˆMulti-Source RAG / Agentï¼‰****ã€‚  
  
## ğŸ§±** äºŒã€æ•´ä½“æ¶æ„ï¼ˆå…³é”®è¦ç†è§£ï¼‰**  
```
ç”¨æˆ· query
       â†“
Query Routerï¼ˆè·¯ç”±ï¼‰
       â†“
æ¯ä¸ªæ•°æ®æºå¯¹åº”ä¸€ä¸ª Retriever / Tool
       â†“
åˆå¹¶æ‰€æœ‰ç»“æœï¼ˆèšåˆï¼‰
       â†“
Rerank + Compressï¼ˆç²¾æ’+å‹ç¼©ï¼‰
       â†“
ä¸Šä¸‹æ–‡ Orchestratorï¼ˆä¸Šä¸‹æ–‡å·¥ç¨‹ï¼‰
       â†“
LLM ç”Ÿæˆæœ€ç»ˆå›ç­”

```
æ ¸å¿ƒç†å¿µï¼š  
****æ•°æ®æºè¦æ‹†åˆ†ï¼Œæ£€ç´¢è¦åˆ†åˆ«åšï¼Œä¸Šä¸‹æ–‡è¦ç»Ÿä¸€ orchestrateã€‚****  
****æ•°æ®æºè¦æ‹†åˆ†ï¼Œæ£€ç´¢è¦åˆ†åˆ«åšï¼Œä¸Šä¸‹æ–‡è¦ç»Ÿä¸€ orchestrateã€‚****  
  
## ğŸ§©** ä¸‰ã€ä»£ç ç»“æ„ï¼ˆæœ€ä½³æ¨¡æ¿ï¼‰**  
```
multi_source_rag/
 â”œâ”€â”€ retrievers/
 â”‚    â”œâ”€â”€ vector_retriever.py   # æ–‡æ¡£å‘é‡åº“
 â”‚    â”œâ”€â”€ sql_retriever.py      # SQL æ•°æ®åº“
 â”‚    â”œâ”€â”€ web_retriever.py      # ç½‘ç»œæ•°æ®/API
 â”‚    â””â”€â”€ wiki_retriever.py     # Wikipedia
 â”œâ”€â”€ router/
 â”‚    â””â”€â”€ query_router.py       # Query Router
 â”œâ”€â”€ aggregator/
 â”‚    â””â”€â”€ aggregator.py         # åˆå¹¶å¤šä¸ªæ£€ç´¢ç»“æœ
 â”œâ”€â”€ orchestrator/
 â”‚    â””â”€â”€ context_manager.py    # ä¸Šä¸‹æ–‡å·¥ç¨‹
 â”œâ”€â”€ main.py                    # ä¸»è°ƒç”¨å…¥å£
 â””â”€â”€ config.py

```
è¿™æ˜¯ä¼ä¸šçº§ç»“æ„ã€‚  
  
## ğŸ”** å››ã€æ„å»ºå„ä¸ªæ•°æ®æº Retrieverï¼ˆæ ¸å¿ƒï¼‰**  
## **1. æ–‡æ¡£å‘é‡åº“ Retriever**  
```
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

def build_vector_retriever():
    embed = OpenAIEmbeddings(model="text-embedding-3-large")
    store = Chroma(persist_directory="./db", embedding_function=embed)
    return store.as_retriever(search_kwargs={"k": 5})

```
  
## **2. SQL æ•°æ®åº“ Retriever**  
ï¼ˆæŸ¥è¯¢ç»“æœè¿”å›æ–‡æœ¬ Documentï¼‰  
```
from langchain import SQLDatabase
from langchain.chains import SQLDatabaseChain

def build_sql_retriever(llm):
    db = SQLDatabase.from_uri("sqlite:///sales.db")
    chain = SQLDatabaseChain(llm=llm, database=db)

    class SQLRetriever:
        def get_relevant_documents(self, query):
            answer = chain.run(query)
            return [Document(page_content=answer, metadata={"source": "sql"})]
    return SQLRetriever()

```
  
## **3. ç½‘ç»œ/API Retrieverï¼ˆå®æ—¶æ•°æ®ï¼‰**  
```
import requests

class WebRetriever:
    def get_relevant_documents(self, query):
        response = requests.get(
            "https://api.duckduckgo.com/",
            params={"q": query, "format": "json"}
        ).json()
        text = response.get("Abstract", "")
        return [Document(page_content=text, metadata={"source": "web"})]

```
  
## **4. Wikipedia Retriever**  
```
from langchain.document_loaders import WikipediaLoader

class WikiRetriever:
    def get_relevant_documents(self, query):
        docs = WikipediaLoader(query=query, load_max_docs=2).load()
        return docs

```
  
## ğŸ”€** äº”ã€Query Routerï¼ˆå†³å®šç”¨å“ªä¸ªæ•°æ®æºï¼‰**  
LangChain å†…ç½® RouterChainã€‚  
ä¾‹å­ï¼š  
```
from langchain.chains.router import MultiPromptChain, RouterChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

def build_router(llm):
    destinations = {
        "docs": "ä¸å†…éƒ¨æ–‡æ¡£ã€å…¬å¸æ”¿ç­–ã€PDF ç›¸å…³çš„å†…å®¹",
        "sql": "ä¸é”€å”®ã€è´¢åŠ¡ã€æ•°é‡ç»Ÿè®¡ã€æ•°æ®åˆ†æç›¸å…³çš„é—®é¢˜",
        "web": "æ¶‰åŠå®æ—¶æ•°æ®ã€å¤–éƒ¨ä¿¡æ¯ã€æ–°é—»çš„æŸ¥è¯¢",
        "wiki": "éœ€è¦ç™¾ç§‘çŸ¥è¯†ã€èƒŒæ™¯èµ„æ–™çš„æŸ¥è¯¢"
    }

    router_prompt = """
ç”¨æˆ·é—®é¢˜: {input}

ç›®æ ‡:
åˆ¤æ–­ç”¨æˆ·çš„æ„å›¾å±äºä»¥ä¸‹å“ªä¸ªæ•°æ®æºï¼š
{destinations}

åªè¿”å›ä¸€ä¸ª keyã€‚
"""

    router_chain = LLMChain(
        llm=llm,
        prompt=PromptTemplate(
            input_variables=["input", "destinations"],
            template=router_prompt
        )
    )
    return router_chain

```
  
## ğŸ”—** å…­ã€èšåˆå¤šä¸ªæ•°æ®æºï¼ˆå¯å¹¶è¡Œï¼‰**  
```
def aggregate_results(results_list):
    all_docs = []
    for docs in results_list:
        all_docs.extend(docs)
    return all_docs

```
å¦‚æœä½ å¸Œæœ›å¹¶è¡Œï¼š  
```
import asyncio

async def multi_query(query, retrievers):
    tasks = [r.get_relevant_documents(query) for r in retrievers]
    results = await asyncio.gather(*tasks)
    return aggregate_results(results)

```
  
## ğŸ›** ä¸ƒã€ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ˆrerank + å‹ç¼©ï¼‰**  
```
from langchain.retrievers.context_compressor import ContextualCompressionRetriever
from langchain_community.document_transformers import BGERerank

def compress(docs, question):
    reranker = BGERerank(top_n=3)
    compressor = ContextualCompressionRetriever(
        base_compressor=reranker,
        base_retriever=None
    )
    return compressor.compress_documents(docs, query=question)

```
  
## ğŸ§ ** å…«ã€æœ€ç»ˆå¤§æ¨¡å‹ç”Ÿæˆå›ç­”ï¼ˆDeepSeek/Qwen/OpenAIï¼‰**  
```
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="deepseek-chat", temperature=0)

def final_answer(question, docs):
    context = "\n\n".join([d.page_content for d in docs])

    prompt = f"""
ä½ æ˜¯ä¸€åä¸“ä¸šçŸ¥è¯†èåˆ AIã€‚

æ ¹æ®ä¸‹é¢æ¥è‡ªå¤šä¸ªæ•°æ®æºçš„å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š
{question}

ç›¸å…³å†…å®¹ï¼š
{context}

è¯·ç»™å‡ºï¼š
1. æ˜ç¡®ç»“è®º  
2. å„æ•°æ®æºçš„ä¾æ®  
3. æ¨ç†è¿‡ç¨‹  
4. æœ€ç»ˆå»ºè®®ï¼ˆå¦‚é€‚ç”¨ï¼‰
"""

    return llm.invoke(prompt).content

```
  
## ğŸ”¥** ä¹ã€å¤šæ•°æ®æºé›†æˆä¸»å‡½æ•°**  
```
def rag_multi_source(question):
    retrievers = [
        vector_retriever,
        sql_retriever,
        web_retriever,
        wiki_retriever,
    ]

    # å¤šæºæ£€ç´¢
    results = []
    for r in retrievers:
        docs = r.get_relevant_documents(question)
        results.append(docs)

    # åˆå¹¶
    all_docs = aggregate_results(results)

    # ç²¾æ’ä¸å‹ç¼©
    selected_docs = compress(all_docs, question)

    # ç”Ÿæˆæœ€ç»ˆå›ç­”
    return final_answer(question, selected_docs)

```
è°ƒç”¨ï¼š  
```
print(rag_multi_source("æˆ‘ä»¬ 2024 å¹´é”€å”®ä¸‹é™çš„åŸå› ï¼Ÿç»“åˆè¡Œä¸šæ•°æ®åˆ†æ"))

```
  
## ğŸ†** åã€ä¼ä¸šçº§å»ºè®®ï¼ˆéå¸¸å…³é”®ï¼‰**  
1. **å¤šæºæ£€ç´¢å¿…é¡»ç²¾æ’ï¼Œä¸ç„¶ä¸Šä¸‹æ–‡çˆ†ç‚¸**  
2. **SQL å¿…é¡»åšå®‰å…¨è¿‡æ»¤ï¼ˆç¦æ­¢ DROP/DELETEï¼‰**  
3. **ç½‘ç»œæ£€ç´¢ä¸è¦ç›´æ¥ä¿¡ä»»ï¼Œå¿…é¡» rerank**  
4. **Router ç²¾å‡†åº¦å¾ˆé‡è¦ï¼Œå»ºè®®ç”¨ Qwen2.5/DeepSeek R1 æ¥è·¯ç”±**  
5. **ä¸åŒæ•°æ®æº metadata å¿…é¡»ä¿ç•™ï¼ˆç”¨äºæº¯æºï¼‰**  
6. **å¼ºåˆ¶å‹ç¼©ä¸Šä¸‹æ–‡ï¼Œé¿å… token è¿‡é•¿**  
7. **ä¸€å®šè¦åšç¼“å­˜ï¼ˆåŒæ · query ä¸è¦é‡å¤æŸ¥ï¼‰**  
  
## ğŸ“Œ** æœ€ç»ˆæ€»ç»“ä¸€å¥è¯**  
****å¤šæ•°æ®æº RAG = å¤šæ£€ç´¢å™¨ + Query Router + èšåˆ + ç²¾æ’å‹ç¼© + LLM Orchestratorã€‚â€¨çœŸæ­£çš„éš¾ç‚¹ä¸æ˜¯æ£€ç´¢ï¼Œæ˜¯â€œå¦‚ä½•æ•´åˆã€è¿‡æ»¤ã€å‹ç¼©å¹¶è°ƒåº¦ä¸Šä¸‹æ–‡â€ã€‚****  
  
å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å†ç»™ä½ ï¼š  
## âœ”** å®Œæ•´é¡¹ç›®è„šæ‰‹æ¶ï¼ˆå«ç›®å½• + æ‰€æœ‰ä»£ç ï¼‰**  
## âœ”** DeepSeek + Multi-Source RAG æœ€ä¼˜æ¶æ„å›¾**  
## âœ”** ä¼ä¸šçº§å¤šæ•°æ®æºä¸­å°ï¼ˆRAG 3.0ï¼‰è®¾è®¡æ–‡æ¡£**  
## âœ”** Streamlit / FastAPI å¯è§†åŒ–ç‰ˆæœ¬**  
ä½ æƒ³è¦å“ªä¸€ä¸ªï¼Ÿ  

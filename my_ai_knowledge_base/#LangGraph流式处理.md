# #LangGraphæµå¼å¤„ç†** **  
ä¸‹é¢æˆ‘ç»™ä½ ä¸€ä»½ ****å·¥ç¨‹çº§ã€å¯ç›´æ¥ä¸Šæ‰‹çš„ã€ŠLangGraph æµå¼å¤„ç†ï¼ˆStreamingï¼‰æœ€ä½³å®è·µæŒ‡å—ã€‹****ã€‚â€¨é‡ç‚¹æ˜¯ï¼š****ä¸æ˜¯å‘Šè¯‰ä½  LangGraph èƒ½æµå¼ï¼Œè€Œæ˜¯è®©ä½ ä¼šå†™ã€ä¼šè°ƒã€ä¼šç”¨åœ¨çœŸå® RAG / Agent / å¤šèŠ‚ç‚¹å·¥ä½œæµä¸­ã€‚****  
LangGraph çš„æµå¼èƒ½åŠ›å…¶å®æ¯” LangChain å¼ºå¾—å¤šï¼Œå› ä¸ºå®ƒå¤©ç„¶æ”¯æŒï¼š  
* èŠ‚ç‚¹çº§åˆ«çš„ streaming  
* å¤šèŠ‚ç‚¹è¿ç»­ streaming  
* äººæœºäº¤äº’ + streaming  
* å·¥å…·è°ƒç”¨ + streaming  
* ä¸­æ–­ + resume + streaming  
* å®Œæ•´çš„ token-by-token è¾“å‡º  
ä¸‹é¢æˆ‘ç”¨æœ€æ¸…æ™°çš„æ–¹å¼è®²é€ã€‚  
  
## ğŸš€** ä¸€ã€LangGraph æµå¼å¤„ç†çš„æœ¬è´¨**  
ä¸€å¥è¯ï¼š  
**LangGraph çš„æµå¼è¾“å‡ºä¸æ˜¯åªæµ LLM tokenï¼Œè€Œæ˜¯æµâ€œäº‹ä»¶ï¼ˆEventsï¼‰â€ï¼Œæ¯ä¸ªäº‹ä»¶ä»£è¡¨èŠ‚ç‚¹æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä¸€æ­¥ã€‚**  
**LangGraph çš„æµå¼è¾“å‡ºä¸æ˜¯åªæµ LLM tokenï¼Œè€Œæ˜¯æµâ€œäº‹ä»¶ï¼ˆEventsï¼‰â€ï¼Œæ¯ä¸ªäº‹ä»¶ä»£è¡¨èŠ‚ç‚¹æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä¸€æ­¥ã€‚**  
äº‹ä»¶åŒ…æ‹¬ï¼š  
* LLM çš„ token è¾“å‡º  
* æŸä¸ªèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ  
* æŸä¸ªèŠ‚ç‚¹ç»“æŸæ‰§è¡Œ  
* å·¥å…·è¿”å›ç»“æœ  
* ä¸­æ–­ï¼ˆéœ€è¦ç”¨æˆ·è¾“å…¥ï¼‰  
* çŠ¶æ€æ›´æ–°ï¼ˆstate mutationï¼‰  
* æ•´ä¸ªå·¥ä½œæµç»“æŸ  
æ‰€ä»¥ï¼ŒLangGraph çš„ streaming æ˜¯ ****çœŸæ­£çš„å·¥ä½œæµ streaming****ï¼Œä¸æ˜¯â€œä»…æµæ¨¡å‹è¾“å‡ºâ€ã€‚  
  
## ğŸ§±** äºŒã€å…³é”® APIï¼ˆå¿…é¡»æŒæ¡ï¼‰**  
## âœ”** app.stream(input, config)**  
****æ‰§è¡Œå›¾ + æµå¼äº‹ä»¶è¾“å‡º****  
## âœ”** app.astream_events(input, config)**  
****æ›´åº•å±‚çš„äº‹ä»¶çº§æµå¼æ¥å£ï¼ˆå»ºè®®ç”¨äºå¤æ‚ agentï¼‰****  
## âœ”** app.send(input, config)**  
****ç”¨äºç”¨æˆ·äº‹ä»¶æ¢å¤ï¼ˆHITLï¼‰æ—¶ï¼Œä¹Ÿæ”¯æŒ streaming****  
  
## ğŸ§ª** ä¸‰ã€æœ€å°å¯è¿è¡Œçš„æµå¼ Demoï¼ˆä¸€ä¸ªèŠ‚ç‚¹ï¼‰**  
ä¸‹é¢ç¤ºä¾‹å¯ä»¥åœ¨çº¿æ€§è¾“å‡º DeepSeek/Llama3 tokenï¼š  
```
from langgraph.graph import StateGraph, END
from typing import List, TypedDict
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="deepseek-chat", streaming=True)

class State(TypedDict):
    messages: List

def chat_node(state: State):
    response = llm.stream(state["messages"])
    for token in response:
        yield token
    return {"messages": state["messages"] + [token]}

graph = StateGraph(State)
graph.add_node("chat", chat_node)
graph.set_entry_point("chat")
graph.add_edge("chat", END)

app = graph.compile()

```
## ****æµå¼è¿è¡Œï¼š****  
```
for event in app.stream(
    {"messages": [HumanMessage(content="Hello!")]},
    config={"thread_id": "1"}
):
    print(event)

```
ä½ ä¼šçœ‹åˆ°ï¼š  
```
{'node': 'chat', 'event': 'start'}
token: "ä½ "
token: "å¥½"
token: "ï¼Œ"
token: "æœ‰"
token: "ä»€"
token: "ä¹ˆ"
...
{'node': 'chat', 'event': 'end'}

```
è¿™æ˜¯ ****å®Œæ•´èŠ‚ç‚¹çº§ streaming****ï¼ˆå¾ˆå¤šæ¡†æ¶åšä¸åˆ°ï¼‰ã€‚  
  
## ğŸ§±** å››ã€æ¨èä½¿ç”¨ï¼šastream_events()ï¼ˆæ›´å¼ºã€æ›´ä¸“ä¸šï¼‰**  
è¿™ä¸ªæ¥å£æä¾› å…¨äº‹ä»¶ æµï¼š  
```
async for event in app.astream_events(
    input,
    config
):
    print(event)

```
å®ƒä¼šç»™ä½ ï¼š  

| äº‹ä»¶ç±»å‹         | ç”¨é€”        |
| ------------ | --------- |
| on_start     | èŠ‚ç‚¹å¼€å§‹      |
| on_end       | èŠ‚ç‚¹ç»“æŸ      |
| on_token     | LLM token |
| on_message   | å·¥å…·/LLM æ¶ˆæ¯ |
| on_interrupt | ç­‰å¾…ç”¨æˆ·è¾“å…¥    |
  
è¿™å¯¹äºï¼š  
* å¤šèŠ‚ç‚¹ Agent  
* RAG + è¯„ä¼°  
* Planner / Tool / Executor  
* è°ƒè¯•å¤æ‚ workflow  
* å‰ç«¯å®æ—¶æ¸²æŸ“  
éå¸¸å…³é”®ã€‚  
  
## ğŸ§ª** äº”ã€å¤æ‚å·¥ä½œæµæµå¼ç¤ºä¾‹ï¼ˆå¤šèŠ‚ç‚¹ï¼‰**  
ä»¥ä¸‹æ˜¯ä¸€ä¸ª****å¤šèŠ‚ç‚¹ RAG æµå¼ pipeline****ï¼š  
1. query_preprocess  
2. retrieve  
3. rerank  
4. generate_answerï¼ˆæµå¼è¾“å‡º tokenï¼‰  
ä»£ç ç»“æ„ï¼š  
```
async for event in app.astream_events(
    {"question": "ä»‹ç»ä¸‹ LangGraph"},
    config={"thread_id": "u_01"}
):
    if event["type"] == "on_token":
        print(event["token"], end="")
    else:
        print("\n[EVENT]", event)

```
è¾“å‡ºç±»ä¼¼ï¼š  
```
[EVENT] on_start: retrieve
[EVENT] retrieved_docs: 3 items
[EVENT] on_end: retrieve
[EVENT] on_start: rerank
[EVENT] rerank_scores: [...]
[EVENT] on_end: rerank

L  # token
a
n
g
G
r
a
p
h
...

```
è¿™æ˜¯ ****çœŸæ­£çš„ Workflow-level Streaming****ã€‚  
  
## ğŸ”¥** å…­ã€äººæœºäº¤äº’ + æµå¼ï¼ˆæœ€å¸¸ç”¨åœºæ™¯ï¼‰**  
å½“ workflow è·‘åˆ°ä¸€ä¸ªèŠ‚ç‚¹ï¼š  
```
raise Interrupt("need_user_input")

```
å‰ç«¯ä¼šæ”¶åˆ°ï¼š  
```
[EVENT] interrupt

```
ç„¶åç”¨æˆ·è¾“å…¥å†…å®¹åï¼š  
```
async for event in app.send(
    {"user_input": "..."},
    config={"thread_id": "u01"}
):
    ...

```
****ç»§ç»­æµå¼è¾“å‡º token****ã€‚  
  
## ğŸ§ ** ä¸ƒã€å·¥ç¨‹çº§æœ€ä½³å®è·µ**  
## âœ”** 1ï¼‰æ‰€æœ‰ LLM èŠ‚ç‚¹éƒ½æ‰“å¼€ streaming**  
```
ChatOpenAI(model="deepseek-chat", streaming=True)

```
## âœ”** 2ï¼‰ä½¿ç”¨ astream_events è€Œä¸æ˜¯ stream**  
ç‰¹åˆ«æ˜¯å¤šèŠ‚ç‚¹ã€å¤š Agentã€è¦å‰ç«¯æ¸²æŸ“çš„åº”ç”¨ã€‚  
## âœ”** 3ï¼‰ç”¨äº‹ä»¶ç±»å‹åŒºåˆ† UI æ“ä½œ**  
ä¾‹å¦‚ï¼š  

| äº‹ä»¶        | å‰ç«¯å¤„ç†     |
| --------- | -------- |
| on_token  | è¾“å‡ºåˆ°èŠå¤©æ¡†   |
| on_start  | æ˜¾ç¤ºâ€œæ‰§è¡Œä¸­â€¦â€ |
| on_end    | æ˜¾ç¤ºâ€œå®Œæˆâ€   |
| interrupt | æ˜¾ç¤ºè¡¨å•ã€æŒ‰é’®  |
  
âœ”**** 4ï¼‰ç»“åˆ Checkpointer åšâ€œæµå¼ + å¯æ¢å¤â€****  
æ”¯æŒï¼š  
* å‰ç«¯é•¿é“¾è·¯ä»»åŠ¡  
* ä¸­æ–­åæ¢å¤  
* å®¡è®¡å›æ”¾  
## âœ”** 5ï¼‰ç»“åˆ RAG Flow**  
retriever â†’ rerank â†’ generator â†’ stream tokenâ€¨æ˜¯æœ€ç»å…¸çš„ RAG äº¤äº’æ–¹å¼ã€‚  
  
## ğŸ¯** å…«ã€æ€»ç»“ï¼šLangGraph çš„æµå¼ä¸ºä»€ä¹ˆå¼ºï¼Ÿ**  
å› ä¸ºå®ƒä¸æ˜¯ model streamingï¼Œè€Œæ˜¯ï¼š  
## âœ”** Workflow Streaming**  
## âœ”** Node Streaming**  
## âœ”** Event Streaming**  
## âœ”** Human-in-the-Loop Streaming**  
## âœ”** Multi-node, multi-agent Streaming**  
## âœ”** Recoverable Streaming**  
ä¸€å¥è¯ï¼š  
**LangChain æµçš„æ˜¯ tokenï¼ŒLangGraph æµçš„æ˜¯â€œæ™ºèƒ½ä½“çš„æ‰§è¡Œå…¨è¿‡ç¨‹â€ã€‚**  
**LangChain æµçš„æ˜¯ tokenï¼ŒLangGraph æµçš„æ˜¯â€œæ™ºèƒ½ä½“çš„æ‰§è¡Œå…¨è¿‡ç¨‹â€ã€‚**  
è¿™å°±æ˜¯å®ƒä¸ºä»€ä¹ˆèƒ½åšâ€œå¯æ§ Agentâ€ã€‚  
  
å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­ç»™ä½ ï¼š  
## ğŸ”¹** LangGraph + å‰ç«¯ï¼ˆNext.jsï¼‰å®æ—¶æµå¼ Demo**  
## ğŸ”¹** LangGraph + DeepSeek R1 Streamingï¼ˆå«æ€ç»´é“¾è¿‡æ»¤ï¼‰**  
## ğŸ”¹** LangGraph å¤š Agent Streaming å·¥ä½œæµ**  
## ğŸ”¹** LangGraph å®ç° GPTs çš„ Streaming ç­‰ä»·èƒ½åŠ›**  
ä½ æƒ³è¦å“ªä¸€ä¸ªï¼Ÿ  
